{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-02-17T06:41:44.904702Z",
     "start_time": "2024-02-17T06:41:43.210404400Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1/1] unbalance\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '../data/raw\\\\unbalance\\\\unbalance-gt.pa'",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mFileNotFoundError\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[1], line 59\u001B[0m\n\u001B[0;32m     57\u001B[0m labels_file_name \u001B[38;5;241m=\u001B[39m \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mFILE_NAME\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m-gt.pa\u001B[39m\u001B[38;5;124m'\u001B[39m\n\u001B[0;32m     58\u001B[0m labels_true_path \u001B[38;5;241m=\u001B[39m os\u001B[38;5;241m.\u001B[39mpath\u001B[38;5;241m.\u001B[39mjoin(raw_data_path, DATASET_DIR, labels_file_name)\n\u001B[1;32m---> 59\u001B[0m labels_true \u001B[38;5;241m=\u001B[39m \u001B[43mload_labels_from_file\u001B[49m\u001B[43m(\u001B[49m\u001B[43mlabels_true_path\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m15\u001B[39;49m\u001B[43m)\u001B[49m\n\u001B[0;32m     60\u001B[0m N_CLUSTERS \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mlen\u001B[39m(\u001B[38;5;28mset\u001B[39m(labels_true))\n\u001B[0;32m     62\u001B[0m \u001B[38;5;66;03m# Placeholder for clustering and evaluation logic\u001B[39;00m\n\u001B[0;32m     63\u001B[0m \u001B[38;5;66;03m# Here you should include your logic for hyperparameter tuning, clustering, and evaluation\u001B[39;00m\n\u001B[0;32m     64\u001B[0m \u001B[38;5;66;03m# This might involve training a KMeans model, evaluating its performance, and then saving the results\u001B[39;00m\n\u001B[0;32m     65\u001B[0m \n\u001B[0;32m     66\u001B[0m \u001B[38;5;66;03m# Define the path for saving metrics\u001B[39;00m\n",
      "File \u001B[1;32m~\\PycharmProjects\\cluster-baseline-analysis\\src\\utils_clustering_v2.py:254\u001B[0m, in \u001B[0;36mload_labels_from_file\u001B[1;34m(file_path, labels_pred_len)\u001B[0m\n\u001B[0;32m    247\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mload_labels_from_file\u001B[39m(file_path, labels_pred_len):\n\u001B[0;32m    248\u001B[0m \u001B[38;5;250m    \u001B[39m\u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[0;32m    249\u001B[0m \u001B[38;5;124;03m    Loads clustering labels from a text file, ignoring the header and metadata.\u001B[39;00m\n\u001B[0;32m    250\u001B[0m \n\u001B[0;32m    251\u001B[0m \u001B[38;5;124;03m    :param file_path: Path to the file containing the labels.\u001B[39;00m\n\u001B[0;32m    252\u001B[0m \u001B[38;5;124;03m    :return: List of labels as integers.\u001B[39;00m\n\u001B[0;32m    253\u001B[0m \u001B[38;5;124;03m    \"\"\"\u001B[39;00m\n\u001B[1;32m--> 254\u001B[0m     \u001B[38;5;28;01mwith\u001B[39;00m \u001B[38;5;28;43mopen\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43mfile_path\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43mr\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m)\u001B[49m \u001B[38;5;28;01mas\u001B[39;00m file:\n\u001B[0;32m    255\u001B[0m         lines \u001B[38;5;241m=\u001B[39m file\u001B[38;5;241m.\u001B[39mreadlines()\n\u001B[0;32m    257\u001B[0m     \u001B[38;5;66;03m# Skipping the header and metadata, start reading from the line after '-----'\u001B[39;00m\n",
      "\u001B[1;31mFileNotFoundError\u001B[0m: [Errno 2] No such file or directory: '../data/raw\\\\unbalance\\\\unbalance-gt.pa'"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "import warnings\n",
    "# Assuming these modules are correctly defined in your project\n",
    "from src.utils_clustering_v2 import load_labels_from_file, map_clusters_to_ground_truth, evaluate_clustering, generate_confusion_matrix\n",
    "from src.data_preprocessing import preprocess_data\n",
    "\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "n_iter = 60\n",
    "VERSION = '0.1.3'\n",
    "DATASET_DIR = 'unbalance'\n",
    "ALGORITHM_NAME = 'KMeans_clustering'  # Added for clarity in directory structure\n",
    "\n",
    "# Ensure directories are created for metric logs\n",
    "def ensure_dir(file_path):\n",
    "    directory = os.path.dirname(file_path)\n",
    "    if not os.path.exists(directory):\n",
    "        os.makedirs(directory, exist_ok=True)\n",
    "\n",
    "# Loop over datasets\n",
    "directory_path = f'../data/raw/{DATASET_DIR}'\n",
    "files = [f for f in os.listdir(directory_path) if os.path.isfile(os.path.join(directory_path, f))]\n",
    "total_files = len(files)\n",
    "\n",
    "for index, filename in enumerate(files, start=1):\n",
    "    FILE_NAME = filename.split('.')[0]\n",
    "    print(f\"[{index}/{total_files}] {FILE_NAME}\")\n",
    "    \n",
    "    # Path for processed data\n",
    "    processed_data_path = f'../data/processed/{DATASET_DIR}/{FILE_NAME}.txt'\n",
    "    \n",
    "    # Ensure the processed data directory exists and save the processed data\n",
    "    ensure_dir(processed_data_path)\n",
    "    \n",
    "    # New results path according to the specified structure\n",
    "    results_dir = f'../results/metrics/{ALGORITHM_NAME}/{DATASET_DIR}/{FILE_NAME}'\n",
    "    results_file = f'results_v{VERSION}.csv'\n",
    "    results_path = os.path.join(results_dir, results_file)"
   ]
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import warnings\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.cluster import KMeans\n",
    "import time\n",
    "from sklearn.model_selection import ParameterSampler\n",
    "from src.utils_clustering_v2 import load_labels_from_file, map_clusters_to_ground_truth, evaluate_clustering\n",
    "from src.data_preprocessing import preprocess_data\n",
    "\n",
    "# Ignore all warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "n_iter = 60\n",
    "VERSION = '0.1.3'\n",
    "DATASET_DIRS = ['a1', 'b1', 's1', 'unbalance']\n",
    "\n",
    "for DATASET_DIR in DATASET_DIRS:\n",
    "    # Specify the directory path you want to list files from\n",
    "    directory_path = f'../data/raw/{DATASET_DIR}'\n",
    "    os.makedirs(directory_path, exist_ok=True)  # Ensure the directory exists\n",
    "\n",
    "    # Get a list of all files in the directory (excluding directories)\n",
    "    files = [f for f in os.listdir(directory_path) if os.path.isfile(os.path.join(directory_path, f))]\n",
    "    total_files = len(files)\n",
    "\n",
    "    for index, filename in enumerate(files, start=1):\n",
    "        FILE_NAME = filename.split('.')[0]\n",
    "        print(f\"[{index}/{total_files}] {FILE_NAME}\")\n",
    "\n",
    "        file_path = os.path.join('..', 'data', 'raw', DATASET_DIR, f'{FILE_NAME}.txt')\n",
    "        data = pd.read_csv(file_path, sep=\"\\s+\", header=None, names=['X', 'Y'])\n",
    "        data_clean = data.dropna()\n",
    "        processed_data = preprocess_data(data)\n",
    "\n",
    "        # Visualize initial and processed data (omitted for brevity)\n",
    "\n",
    "        # Ensure the processed data directory exists\n",
    "        processed_directory_path = os.path.join('..', 'data', 'processed', DATASET_DIR)\n",
    "        os.makedirs(processed_directory_path, exist_ok=True)\n",
    "\n",
    "        # Save the processed data to a CSV file\n",
    "        processed_data_path = os.path.join(processed_directory_path, f'{FILE_NAME}.txt')\n",
    "        processed_data.to_csv(processed_data_path, index=False)\n",
    "\n",
    "    # Clustering analysis (simplified for brevity)\n",
    "    # This part of the code should follow a similar pattern:\n",
    "    # 1. Load labels and processed data.\n",
    "    # 2. Perform train/test splits.\n",
    "    # 3. Parameterize and execute the clustering algorithm.\n",
    "    # 4. Evaluate and save clustering results.\n",
    "\n",
    "# Note: The detailed implementation of clustering analysis and evaluation\n",
    "# is omitted here for clarity. Be sure to adapt the paths and logic\n",
    "# according to the steps outlined above.\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "3c533e841582d8ec"
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "data": {
      "text/plain": "array([  5,  10,  15,  20,  25,  30,  35,  40,  45,  50,  55,  60,  65,\n        70,  75,  80,  85,  90,  95, 100])"
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "np.arange(5, 105, 5)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-18T00:18:02.416742Z",
     "start_time": "2024-02-18T00:18:02.406248800Z"
    }
   },
   "id": "5de5de94aa5324db",
   "execution_count": 3
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "data": {
      "text/plain": "'C:\\\\Users\\\\akila\\\\PycharmProjects\\\\cluster-baseline-analysis\\\\notebooks'"
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "current_directory = os.getcwd()\n",
    "current_directory"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-18T08:05:16.559643100Z",
     "start_time": "2024-02-18T08:05:16.528752400Z"
    }
   },
   "id": "76da3890bcacfcb5",
   "execution_count": 5
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
