{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-02-24T11:10:00.472997900Z",
     "start_time": "2024-02-24T11:10:00.454995500Z"
    }
   },
   "outputs": [],
   "source": [
    "# import time\n",
    "# import numpy as np\n",
    "# import pandas as pd\n",
    "# from sklearn.preprocessing import StandardScaler\n",
    "# from sklearn.cluster import KMeans, DBSCAN, AffinityPropagation, Birch, OPTICS, MeanShift, AgglomerativeClustering\n",
    "# from sklearn.cluster import BisectingKMeans\n",
    "# from sklearn.metrics import pairwise_distances\n",
    "# from pyclustering.cluster.fcm import fcm\n",
    "# from pyclustering.cluster.cure import cure\n",
    "# from pyclustering.cluster.clique import clique\n",
    "# import warnings\n",
    "# from pyclustering.cluster.fcm import fcm\n",
    "# from pyclustering.cluster.center_initializer import kmeans_plusplus_initializer\n",
    "# from pyclustering.utils.metric import distance_metric, type_metric\n",
    "# from collections import Counter\n",
    "# from src.utils_clustering import *\n",
    "# import ast\n",
    "# \n",
    "# \n",
    "# # Ignore all warnings\n",
    "# warnings.filterwarnings(\"ignore\")\n",
    "# \n",
    "# # Load dataset\n",
    "# DATASET_FILE_NAME = \"s1\"\n",
    "# file_path = rf'../data/processed/{DATASET_FILE_NAME}.txt'\n",
    "# processed_data = pd.read_csv(file_path)\n",
    "# X = processed_data.values\n",
    "# \n",
    "# # Optionally scale the data for algorithms that are distance-based\n",
    "# scaler = StandardScaler()\n",
    "# X_scaled = scaler.fit_transform(X)\n",
    "# \n",
    "# N_CLASSES = 15  # Number of clusters/classes to use for applicable algorithms\n",
    "# \n",
    "# LABELS_FILE_NAME = 's1-label'\n",
    "# labels_true = load_labels_from_file(rf'..\\data\\label\\{LABELS_FILE_NAME}.pa', N_CLASSES)\n",
    "# dataset_name = DATASET_FILE_NAME\n",
    "# results_path = r'../results/metrics/results.csv'\n",
    "# \n",
    "# # Calculate the preference\n",
    "# # This uses the median of the pairwise distances as a starting point\n",
    "# # You might need to adjust this based on the performance\n",
    "# pairwise_dist = pairwise_distances(processed_data)\n",
    "# preference = np.percentile(pairwise_dist, 50) * -1\n",
    "# \n",
    "# # Calculate the bandwidth\n",
    "# pairwise_dist = pairwise_distances(processed_data)\n",
    "# quantile_value = 0.055\n",
    "# bandwidth = np.quantile(pairwise_dist, quantile_value)\n",
    "# \n",
    "# # Define clustering algorithms to run\n",
    "# clustering_algorithms = [\n",
    "#     ('KMeans', KMeans(n_clusters=8, init='k-means++', n_init='auto', max_iter=300, tol=1e-4, algorithm='lloyd'),\n",
    "#      X_scaled),\n",
    "#     ('DBSCAN', DBSCAN(eps=0.5, min_samples=5), X_scaled),\n",
    "#     ('AffinityPropagation',\n",
    "#      AffinityPropagation(damping=0.5, max_iter=2000, convergence_iter=30, preference=None, affinity='euclidean'),\n",
    "#      X_scaled),\n",
    "#     ('BIRCH', Birch(threshold=0.5, branching_factor=50, n_clusters=5), X_scaled),\n",
    "#     # FCM initialized later due to special requirements\n",
    "#     ('Fuzzy C Means', None, X),\n",
    "#     ('OPTICS', OPTICS(min_samples=5, max_eps=np.inf, metric='minkowski', p=2, cluster_method='xi', xi=0.05,\n",
    "#                       min_cluster_size=None, ),\n",
    "#      X_scaled),\n",
    "#     ('Mean Shift', MeanShift(bandwidth=None, bin_seeding=None, min_bin_freq=1, cluster_all=True, max_iter=300),\n",
    "#      X_scaled),\n",
    "#     ('AgglomerativeClustering', AgglomerativeClustering(n_clusters=2, metric='euclidean', linkage='ward'), X_scaled),\n",
    "#     ('Bisecting KMeans',\n",
    "#      BisectingKMeans(n_clusters=8, init='random', n_init=1, max_iter=300, tol=1e-4, algorithm='lloyd',\n",
    "#                      bisecting_strategy='biggest_inertia'), X_scaled),\n",
    "#     # CURE and CLIQUE from pyclustering require different handling\n",
    "#     ('CURE', cure(X.tolist(), number_cluster=2, number_represent_points=5, compression=0.5), X),\n",
    "#     ('CLIQUE', clique(X.tolist(), amount_intervals=2, density_threshold=2), X)\n",
    "# ]\n",
    "# \n",
    "# for name, algorithm, dataset in clustering_algorithms:\n",
    "#     print(f\"\\nProcessing {name}...\")\n",
    "# \n",
    "#     if name == 'Fuzzy C Means':\n",
    "#         # Assuming 'dataset' is your data prepared for clustering\n",
    "#         data = pd.read_csv(file_path, header=None, sep='\\s+', names=['X', 'Y'])\n",
    "# \n",
    "#         # Assuming the dataset is already in a suitable format (two columns for X and Y coordinates)\n",
    "#         # If your dataset includes headers or other non-numeric rows, make sure to preprocess it accordingly.\n",
    "#         processed_data = data.values.tolist()\n",
    "#         if processed_data[0][0] == 'X,Y':\n",
    "#             raw_data = processed_data[1:]\n",
    "# \n",
    "#         # Split the string by comma and convert to float, ignore the second column as it's NaN\n",
    "#         processed_data = [[float(coord) for coord in point[0].split(',')] for point in raw_data]\n",
    "# \n",
    "#         kmeans_init = KMeans(n_clusters=N_CLASSES, init='k-means++', n_init=1, random_state=42)\n",
    "#         kmeans_init.fit(dataset)\n",
    "# \n",
    "#         initial_centers = kmeans_init.cluster_centers_\n",
    "#         try:\n",
    "#             initial_centers = kmeans_plusplus_initializer(processed_data, N_CLASSES).initialize()\n",
    "#         except Exception as e:\n",
    "#             pass\n",
    "#             # print(\"An error occurred during k-means++ initialization:\", e)\n",
    "# \n",
    "#         metric = distance_metric(type_metric.EUCLIDEAN)\n",
    "#         fcm_instance = fcm(processed_data, initial_centers, metric=metric, tolerance=0.001, itermax=200, m=2)\n",
    "#         # Run FCM clustering\n",
    "#         start_time = time.time()\n",
    "#         fcm_instance.process()\n",
    "#         end_time = time.time()\n",
    "#         # Get the membership matrix\n",
    "#         membership_matrix = fcm_instance.get_membership()\n",
    "# \n",
    "#         # Assign each data point to the cluster with the highest membership\n",
    "#         labels_pred = np.argmax(membership_matrix, axis=1)\n",
    "#         cluster_distribution = Counter(labels_pred)\n",
    "#     # Special handling for pyclustering algorithms\n",
    "#     elif name in ['CURE', 'CLIQUE']:\n",
    "#         start_time = time.time()\n",
    "#         algorithm.process()\n",
    "#         end_time = time.time()\n",
    "#         clusters = algorithm.get_clusters()\n",
    "#         labels_pred = np.zeros(len(processed_data))\n",
    "# \n",
    "#         for cluster_id, cluster in enumerate(clusters):\n",
    "#             for index in cluster:\n",
    "#                 labels_pred[index] = cluster_id\n",
    "# \n",
    "#         # pyclustering algorithms have different outputs; adapt as needed\n",
    "#         print(f\"{name}: Clusters found (sample) - {len(labels_pred)}\")\n",
    "#     else:\n",
    "#         start_time = time.time()\n",
    "#         algorithm.fit(dataset)\n",
    "#         end_time = time.time()\n",
    "#         if hasattr(algorithm, 'labels_'):\n",
    "#             labels_pred = algorithm.labels_\n",
    "#         else:\n",
    "#             labels_pred = algorithm.predict(dataset)\n",
    "#         print(f\"{name}: Clusters found - {len(set(labels_pred))}\")\n",
    "# \n",
    "#     running_time = end_time - start_time\n",
    "#     running_time_in_seconds = running_time / 60\n",
    "#     print(f\"{name} took {running_time_in_seconds} seconds\")\n",
    "#     cluster_distribution = Counter(labels_pred)\n",
    "# \n",
    "#     # if labels_pred is not None:\n",
    "#     #     print(f\"Predicted Distribution for {name}: {cluster_distribution}\")\n",
    "#     evaluate_clustering(X=processed_data, labels_true=labels_true, labels_pred=labels_pred,\n",
    "#                         clus_algo_name=name, dataset_name=dataset_name,\n",
    "#                         results_path=results_path, algorithm_details=\"\",\n",
    "#                         running_time=running_time_in_seconds)"
   ]
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "data": {
      "text/plain": "array([0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1. ])"
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "np.linspace(0.1, 1.0, 10)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-24T11:10:00.521001300Z",
     "start_time": "2024-02-24T11:10:00.477995700Z"
    }
   },
   "id": "373c8e1800815207",
   "execution_count": 28
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "N_CLUSTERS = 15"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-24T11:10:00.543066700Z",
     "start_time": "2024-02-24T11:10:00.523003600Z"
    }
   },
   "id": "a7e1f0f537a1decb",
   "execution_count": 29
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "hyperparameter_space = {\n",
    "    'KMeans': {\n",
    "        'n_clusters': [N_CLUSTERS],\n",
    "        'init': ['k-means++', 'random'],\n",
    "        'n_init': np.arange(5, 105, 10),\n",
    "        'max_iter': [100, 300, 1000, 5000, 10000, 20000],\n",
    "        'algorithm': ['auto', 'elkan', 'full']\n",
    "    },\n",
    "    'DBSCAN': {\n",
    "        'eps': np.linspace(0.1, 2.0, 20),\n",
    "        'min_samples': list(range(1, 20))\n",
    "    },\n",
    "    'AffinityPropagation': {\n",
    "        'damping': np.linspace(0.5, 0.99, 10),\n",
    "        'max_iter': list(range(200, 3000, 200)),\n",
    "        'convergence_iter': list(range(10, 100, 10)),\n",
    "        'preference': np.linspace(-50, 50, 11),\n",
    "        'affinity': ['euclidean', 'precomputed']\n",
    "    },\n",
    "    'BIRCH': {\n",
    "        'threshold': np.linspace(0.1, 1.0, 10),\n",
    "        'branching_factor': list(range(10, 100, 10)),\n",
    "        'n_clusters': [None] + list(range(2, 50))\n",
    "    },\n",
    "    'Fuzzy C Means': {\n",
    "        'tolerance': [1e-4, 1e-3, 1e-2],\n",
    "        'itermax': list(range(100, 1000, 100)),\n",
    "        'm': np.linspace(1.1, 2.0, 10)\n",
    "    },\n",
    "    'OPTICS': {\n",
    "        'min_samples': [10, 20, 50],\n",
    "        'max_eps': [1, 5, 7.5, np.inf],\n",
    "        'metric': ['euclidean', 'cosine'],\n",
    "        'xi': np.linspace(0.01, 0.1, 5),\n",
    "        'min_cluster_size': [None, 5, 10, 20]\n",
    "    },\n",
    "    'Mean Shift': {\n",
    "        'bandwidth': [0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0],\n",
    "        'bin_seeding': [True, False],\n",
    "        'min_bin_freq': [None, 5, 10, 20],\n",
    "        'cluster_all': [True, False],\n",
    "        'max_iter': [100, 300, 1000, 5000]\n",
    "    },\n",
    "    'AgglomerativeClustering': {\n",
    "        'n_clusters': list(range(2, 50)),\n",
    "        'affinity': ['euclidean', 'l1', 'l2', 'manhattan', 'cosine', 'precomputed'],\n",
    "        'linkage': ['ward']\n",
    "    },\n",
    "    'BisectingKMeans': {\n",
    "        'n_clusters': [N_CLUSTERS],\n",
    "        'init': ['k-means++'],\n",
    "        'n_init': [1, 10, 15, 20],\n",
    "        'tol': [1e-4, 1e-3, 1e-2],\n",
    "        'max_iter': [50, 100, 300, 1000, 5000],\n",
    "        'algorithm': ['lloyd'],\n",
    "        'bisecting_strategy': ['biggest_inertia', 'biggest_cluster']\n",
    "    },\n",
    "    'CURE': {\n",
    "        'number_cluster': [N_CLUSTERS],\n",
    "        'number_represent_points': list(range(1, 15)),\n",
    "        'compression': np.linspace(0.05, 0.95, 19)\n",
    "    },\n",
    "    'CLIQUE': {\n",
    "        'amount_intervals': list(range(2, 20)),\n",
    "        'density_threshold': list(range(1, 10))\n",
    "    }\n",
    "}\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-24T11:10:00.587069300Z",
     "start_time": "2024-02-24T11:10:00.549066700Z"
    }
   },
   "id": "4bc989a39f7b4796",
   "execution_count": 30
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KMeans hyperparameter grid size: 360\n",
      "DBSCAN hyperparameter grid size: 380\n",
      "AffinityPropagation hyperparameter grid size: 27720\n",
      "BIRCH hyperparameter grid size: 4410\n",
      "Fuzzy C Means hyperparameter grid size: 270\n",
      "OPTICS hyperparameter grid size: 480\n",
      "Mean Shift hyperparameter grid size: 512\n",
      "AgglomerativeClustering hyperparameter grid size: 288\n",
      "BisectingKMeans hyperparameter grid size: 120\n",
      "CURE hyperparameter grid size: 266\n",
      "CLIQUE hyperparameter grid size: 162\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from itertools import product\n",
    "\n",
    "\n",
    "# Function to calculate the grid size of the hyperparameter space\n",
    "def calculate_grid_size(space):\n",
    "    # For each parameter, count the number of unique values and multiply them\n",
    "    return np.prod([len(values) for values in space.values()])\n",
    "\n",
    "\n",
    "# Iterate through each algorithm and calculate its hyperparameter grid size\n",
    "for algorithm, space in hyperparameter_space.items():\n",
    "    grid_size = calculate_grid_size(space)\n",
    "    print(f\"{algorithm} hyperparameter grid size: {grid_size}\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-24T11:10:00.587069300Z",
     "start_time": "2024-02-24T11:10:00.567072600Z"
    }
   },
   "id": "2eb4e6d48c277ccf",
   "execution_count": 31
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "N_DIMENSIONS = 2\n",
    "N_CLUSTERS = 15\n",
    "\n",
    "# Generate 20 random exponent values uniformly between -4 and 1 for DBSCAN's eps\n",
    "eps_exponents = np.random.uniform(-4, 1, 20)\n",
    "eps_values = 100 ** eps_exponents\n",
    "\n",
    "preference_exponents = np.random.uniform(-3, 1, 20)  # Choosing an exponent range\n",
    "preference_values = 10 ** preference_exponents\n",
    "\n",
    "hyperparameter_domains = {\n",
    "    'KMeans': {\n",
    "        'n_clusters': [N_CLUSTERS],\n",
    "        'init': ['k-means++', 'random'],\n",
    "        'n_init': [5, 10, 20, 30, 40],\n",
    "        'max_iter': [100, 300, 1000],\n",
    "        'algorithm': ['auto', 'elkan', 'full']\n",
    "    },\n",
    "    'DBSCAN': {\n",
    "        'eps': [4.1827477212013913e-07, 0.000999793310630198, 1.1555685225259388e-06, 0.007625045798919035,\n",
    "                0.30154027296299313, 0.001133874674429879, 0.6848313153681312, 4.534948509703966e-05,\n",
    "                0.000237833340687198, 1.3295589394829136e-05, 1.366970875410476e-07, 0.0021156876523657567,\n",
    "                6.22036135735188e-05, 0.16847200898026155, 0.0014392246158791363, 5.054605554035152e-06,\n",
    "                3.072677903364226e-05, 2.3992122778616145e-07, 0.000514928741735489, 0.0004534644244900038],\n",
    "        # Generated eps_values\n",
    "        'min_samples': [N_DIMENSIONS + 1, 5, 10, 20, 30]  # Adjusting to N_DIMENSIONS + 1 based on dimensionality\n",
    "    },\n",
    "    'AffinityPropagation': {\n",
    "        'damping': np.linspace(0.5, 0.99, 5),\n",
    "        'preference': [0.0035300406061815905, 8.599741565684038, 0.0048075296321666156, 5.541545454090596,\n",
    "                       0.0038650512358458866, 0.24993703970415368, 1.2957001043053533, 0.41659862670811615,\n",
    "                       0.2926393439846283, 0.7350763931819514, 0.05506857807002312, 0.0213792025641085,\n",
    "                       6.082047440017776, 0.04318016062514807, 0.15870282427221336, 1.1181951884543766,\n",
    "                       0.0020053443048871425, 0.0019108280183739136, 0.0029366744252844897, 0.02490896532008261],\n",
    "        #  log-scale generated values\n",
    "    },\n",
    "}"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-24T11:10:00.605160100Z",
     "start_time": "2024-02-24T11:10:00.592068200Z"
    }
   },
   "id": "82157b335c7107d",
   "execution_count": 32
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KMeans hyperparameter grid size: 90\n",
      "DBSCAN hyperparameter grid size: 100\n",
      "AffinityPropagation hyperparameter grid size: 100\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from itertools import product\n",
    "\n",
    "\n",
    "# Function to calculate the grid size of the hyperparameter space\n",
    "\n",
    "\n",
    "def calculate_grid_size(space):\n",
    "    # For each parameter, count the number of unique values and multiply them\n",
    "    return np.prod([len(values) for values in space.values()])\n",
    "\n",
    "\n",
    "# Iterate through each algorithm and calculate its hyperparameter grid size\n",
    "for algorithm, space in hyperparameter_domains.items():\n",
    "    grid_size = calculate_grid_size(space)\n",
    "    print(f\"{algorithm} hyperparameter grid size: {grid_size}\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-24T11:10:00.646037300Z",
     "start_time": "2024-02-24T11:10:00.599069500Z"
    }
   },
   "id": "957ebac022d5b44e",
   "execution_count": 33
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "ef365588fa4d9558"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "N_DIMENSIONS = 2\n",
    "N_CLUSTERS = 15\n",
    "\n",
    "# Exponent value generation technique\n",
    "# Generate 20 random exponent values uniformly between -4 and 1 for DBSCAN's eps\n",
    "eps_exponents = np.random.uniform(-10, 1, 2000)\n",
    "eps_values = 10 ** eps_exponents\n",
    "\n",
    "# For AffinityPropagation's 'preference'\n",
    "preference_exponents = np.random.uniform(-2, 1, 50)  # Choosing an exponent range\n",
    "preference_values = 10 ** preference_exponents\n",
    "\n",
    "# For BIRCH's 'threshold'\n",
    "threshold_exponents = np.random.uniform(-8, 1, 500)  # Generating values between 10^-1 and 10^0\n",
    "threshold_values = 10 ** threshold_exponents\n",
    "\n",
    "hyperparameter_domains = {\n",
    "    'KMeans': {\n",
    "        'n_clusters': [N_CLUSTERS],\n",
    "        'init': ['k-means++', 'random'],\n",
    "        'n_init': [5, 10, 20, 30, 40, 50, 60, 70, 80, 90],\n",
    "        'max_iter': [100, 300, 1000, 1500, 2000, 3000, 4000, 5000],\n",
    "        'algorithm': ['auto', 'elkan', 'full']\n",
    "    },\n",
    "    'DBSCAN': {\n",
    "        'eps': eps_values,  # Generated eps_values\n",
    "        'min_samples':  [1,2] # Adjusting to N_DIMENSIONS + 1 based on dimensionality\n",
    "    },\n",
    "    'AffinityPropagation': {\n",
    "        'damping': np.linspace(0.5, 0.99, 10),\n",
    "        'preference': preference_values,  #  log-scale generated values\n",
    "    },\n",
    "    'BIRCH': {\n",
    "        'threshold': threshold_values,  #  log-scale generated values\n",
    "        'branching_factor': list(range(2, 100, 2)),  # Keeping linear steps\n",
    "        'n_clusters': [N_CLUSTERS]\n",
    "    },\n",
    "    'Fuzzy C Means': {\n",
    "        'tolerance': [1e-4, 1e-3, 1e-2],\n",
    "        'itermax': list(range(100, 1000, 100)),\n",
    "        'm': np.linspace(1.1, 2.0, 10)\n",
    "    },\n",
    "    'OPTICS': {\n",
    "        'min_samples': [10, 20, 50],\n",
    "        'max_eps': [1, 5, 7.5, np.inf],\n",
    "        'metric': ['euclidean', 'cosine'],\n",
    "        'xi': np.linspace(0.01, 0.1, 5),\n",
    "        'min_cluster_size': [None, 5, 10, 20]\n",
    "    },\n",
    "    'Mean Shift': {\n",
    "        'bandwidth': [0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7],\n",
    "        'bin_seeding': [True, False],\n",
    "        'min_bin_freq': [5, 10, 20],\n",
    "        'cluster_all': [True, False],\n",
    "        'max_iter': [100, 300, 1000, 5000]\n",
    "    },\n",
    "    'AgglomerativeClustering': {\n",
    "        'n_clusters': [N_CLUSTERS],\n",
    "        'affinity': ['euclidean'],\n",
    "        'linkage': ['ward']\n",
    "    },\n",
    "     'BisectingKMeans': {\n",
    "        'n_clusters': [N_CLUSTERS],\n",
    "        'init': ['k-means++'],\n",
    "        'n_init': [1, 10, 15, 20],\n",
    "        'tol': [1e-4, 1e-3, 1e-2],\n",
    "        'max_iter': [50, 100, 300, 1000, 5000],\n",
    "        'algorithm': ['lloyd'],\n",
    "        'bisecting_strategy': ['biggest_inertia', 'biggest_cluster']\n",
    "    },\n",
    "    'CURE': {\n",
    "        'number_cluster': [N_CLUSTERS],\n",
    "        'number_represent_points': list(range(1, 15)),\n",
    "        'compression': np.linspace(0.05, 0.95, 19)\n",
    "    },\n",
    "    'CLIQUE': {\n",
    "        'amount_intervals': list(range(2, 20)),\n",
    "        'density_threshold': list(range(1, 10))\n",
    "    }\n",
    "}"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-25T00:51:36.779814200Z",
     "start_time": "2024-02-25T00:51:36.764815500Z"
    }
   },
   "id": "7e930eb5bad6330a",
   "execution_count": 10
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KMeans hyperparameter grid size: 480, n_iter 21.908902300206645 , or 48.0 , or 120.0\n",
      "DBSCAN hyperparameter grid size: 4000, n_iter 63.245553203367585 , or 400.0 , or 1000.0\n",
      "AffinityPropagation hyperparameter grid size: 500, n_iter 22.360679774997898 , or 50.0 , or 125.0\n",
      "BIRCH hyperparameter grid size: 24500, n_iter 156.52475842498527 , or 2450.0 , or 6125.0\n",
      "Fuzzy C Means hyperparameter grid size: 270, n_iter 16.431676725154983 , or 27.0 , or 67.5\n",
      "OPTICS hyperparameter grid size: 480, n_iter 21.908902300206645 , or 48.0 , or 120.0\n",
      "Mean Shift hyperparameter grid size: 336, n_iter 18.33030277982336 , or 33.6 , or 84.0\n",
      "AgglomerativeClustering hyperparameter grid size: 1, n_iter 1.0 , or 0.1 , or 0.25\n",
      "BisectingKMeans hyperparameter grid size: 120, n_iter 10.954451150103322 , or 12.0 , or 30.0\n",
      "CURE hyperparameter grid size: 266, n_iter 16.30950643030009 , or 26.6 , or 66.5\n",
      "CLIQUE hyperparameter grid size: 162, n_iter 12.727922061357855 , or 16.2 , or 40.5\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "def calculate_grid_size(space):\n",
    "    # For each parameter, count the number of unique values and multiply them\n",
    "    return np.prod([len(values) for values in space.values()])\n",
    "\n",
    "\n",
    "# Iterate through each algorithm and calculate its hyperparameter grid size\n",
    "for algorithm, space in hyperparameter_domains.items():\n",
    "    grid_size = calculate_grid_size(space)\n",
    "    print(f\"{algorithm} hyperparameter grid size: {grid_size}, n_iter {math.sqrt(grid_size)} , or {grid_size/10} , or {grid_size/4}\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-25T00:51:39.654597100Z",
     "start_time": "2024-02-25T00:51:39.631597500Z"
    }
   },
   "id": "b50efd2f43bb57dc",
   "execution_count": 11
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DBSCAN hyperparameter grid size: 4000\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "N_DIMENSIONS = 2\n",
    "N_CLUSTERS = 15\n",
    "\n",
    "# Generating a very large range of eps values\n",
    "eps_exponents_very_large = np.random.uniform(-6, 3, 200)  # Even wider range and more samples\n",
    "eps_values_very_large = 100 ** eps_exponents_very_large\n",
    "\n",
    "# Expanding min_samples range significantly\n",
    "min_samples_very_large = np.arange(N_DIMENSIONS + 1, 200, 10).tolist()  # More granular steps\n",
    "\n",
    "very_large_search_domain = {\n",
    "    'DBSCAN': {\n",
    "        'eps': eps_values_very_large,\n",
    "        'min_samples': min_samples_very_large\n",
    "    }\n",
    "}\n",
    "\n",
    "\n",
    "def calculate_grid_size(space):\n",
    "    # For each parameter, count the number of unique values and multiply them\n",
    "    return np.prod([len(values) for values in space.values()])\n",
    "\n",
    "\n",
    "# Iterate through each algorithm and calculate its hyperparameter grid size\n",
    "for algorithm, space in very_large_search_domain.items():\n",
    "    grid_size = calculate_grid_size(space)\n",
    "    print(f\"{algorithm} hyperparameter grid size: {grid_size}\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-24T11:10:00.695038400Z",
     "start_time": "2024-02-24T11:10:00.648036900Z"
    }
   },
   "id": "50ab764254e81e31",
   "execution_count": 36
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DBSCAN hyperparameter grid size: 1200\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Parameters\n",
    "N_DIMENSIONS = 2\n",
    "N_CLUSTERS = 15\n",
    "\n",
    "# Generating a larger range of eps values\n",
    "eps_exponents_large = np.random.uniform(-5, 3, 100)\n",
    "eps_values_large = 10 ** eps_exponents_large\n",
    "\n",
    "# Specifying a wider range of min_samples\n",
    "min_samples_large = [N_DIMENSIONS + 1, 5, 10, 20, 30, 40, 50, 60, 70, 80, 90, 100]\n",
    "\n",
    "large_search_domain = {\n",
    "    'DBSCAN': {\n",
    "        'eps': eps_values_large,\n",
    "        'min_samples': min_samples_large\n",
    "    }\n",
    "}\n",
    "\n",
    "\n",
    "def calculate_grid_size(space):\n",
    "    # For each parameter, count the number of unique values and multiply them\n",
    "    return np.prod([len(values) for values in space.values()])\n",
    "\n",
    "\n",
    "# Iterate through each algorithm and calculate its hyperparameter grid size\n",
    "for algorithm, space in large_search_domain.items():\n",
    "    grid_size = calculate_grid_size(space)\n",
    "    print(f\"{algorithm} hyperparameter grid size: {grid_size}\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-24T11:10:00.697037100Z",
     "start_time": "2024-02-24T11:10:00.661037100Z"
    }
   },
   "id": "d395855795e0bacd",
   "execution_count": 37
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "data": {
      "text/plain": "array([4.76894495e-01, 1.25731989e-02, 1.01723265e-01, ...,\n       4.14696742e-10, 4.06796697e-05, 1.69722863e-06])"
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eps_values"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-24T11:10:00.698036400Z",
     "start_time": "2024-02-24T11:10:00.676038200Z"
    }
   },
   "id": "3f2308141464d2bd",
   "execution_count": 38
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "data": {
      "text/plain": "[5, 10, 15, 20, 25, 30, 35, 40, 45, 50, 55, 60, 65, 70, 75, 80, 85, 90, 95]"
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(range(5, 100, 5))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-24T11:10:00.733289100Z",
     "start_time": "2024-02-24T11:10:00.692036700Z"
    }
   },
   "id": "9feccc553e65d889",
   "execution_count": 39
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "99e96bc3bd8414e1"
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "g2-1-10 (2047, 1)\n",
      "g2-1-100 (2047, 1)\n",
      "g2-1-20 (2047, 1)\n",
      "g2-1-30 (2047, 1)\n",
      "g2-1-40 (2047, 1)\n",
      "g2-1-50 (2047, 1)\n",
      "g2-1-60 (2047, 1)\n",
      "g2-1-70 (2047, 1)\n",
      "g2-1-80 (2047, 1)\n",
      "g2-1-90 (2047, 1)\n",
      "g2-1024-10 (2047, 1024)\n",
      "g2-1024-100 (2047, 1024)\n",
      "g2-1024-20 (2047, 1024)\n",
      "g2-1024-30 (2047, 1024)\n",
      "g2-1024-40 (2047, 1024)\n",
      "g2-1024-50 (2047, 1024)\n",
      "g2-1024-60 (2047, 1024)\n",
      "g2-1024-70 (2047, 1024)\n",
      "g2-1024-80 (2047, 1024)\n",
      "g2-1024-90 (2047, 1024)\n",
      "g2-128-10 (2047, 128)\n",
      "g2-128-100 (2047, 128)\n",
      "g2-128-20 (2047, 128)\n",
      "g2-128-30 (2047, 128)\n",
      "g2-128-40 (2047, 128)\n",
      "g2-128-50 (2047, 128)\n",
      "g2-128-60 (2047, 128)\n",
      "g2-128-70 (2047, 128)\n",
      "g2-128-80 (2047, 128)\n",
      "g2-128-90 (2047, 128)\n",
      "g2-16-10 (2047, 16)\n",
      "g2-16-100 (2047, 16)\n",
      "g2-16-20 (2047, 16)\n",
      "g2-16-30 (2047, 16)\n",
      "g2-16-40 (2047, 16)\n",
      "g2-16-50 (2047, 16)\n",
      "g2-16-60 (2047, 16)\n",
      "g2-16-70 (2047, 16)\n",
      "g2-16-80 (2047, 16)\n",
      "g2-16-90 (2047, 16)\n",
      "g2-2-10 (2047, 2)\n",
      "g2-2-100 (2047, 2)\n",
      "g2-2-20 (2047, 2)\n",
      "g2-2-30 (2047, 2)\n",
      "g2-2-40 (2047, 2)\n",
      "g2-2-50 (2047, 2)\n",
      "g2-2-60 (2047, 2)\n",
      "g2-2-70 (2047, 2)\n",
      "g2-2-80 (2047, 2)\n",
      "g2-2-90 (2047, 2)\n",
      "g2-256-10 (2047, 256)\n",
      "g2-256-100 (2047, 256)\n",
      "g2-256-20 (2047, 256)\n",
      "g2-256-30 (2047, 256)\n",
      "g2-256-40 (2047, 256)\n",
      "g2-256-50 (2047, 256)\n",
      "g2-256-60 (2047, 256)\n",
      "g2-256-70 (2047, 256)\n",
      "g2-256-80 (2047, 256)\n",
      "g2-256-90 (2047, 256)\n",
      "g2-32-10 (2047, 32)\n",
      "g2-32-100 (2047, 32)\n",
      "g2-32-20 (2047, 32)\n",
      "g2-32-30 (2047, 32)\n",
      "g2-32-40 (2047, 32)\n",
      "g2-32-50 (2047, 32)\n",
      "g2-32-60 (2047, 32)\n",
      "g2-32-70 (2047, 32)\n",
      "g2-32-80 (2047, 32)\n",
      "g2-32-90 (2047, 32)\n",
      "g2-4-10 (2047, 4)\n",
      "g2-4-100 (2047, 4)\n",
      "g2-4-20 (2047, 4)\n",
      "g2-4-30 (2047, 4)\n",
      "g2-4-40 (2047, 4)\n",
      "g2-4-50 (2047, 4)\n",
      "g2-4-60 (2047, 4)\n",
      "g2-4-70 (2047, 4)\n",
      "g2-4-80 (2047, 4)\n",
      "g2-4-90 (2047, 4)\n",
      "g2-512-10 (2047, 512)\n",
      "g2-512-100 (2047, 512)\n",
      "g2-512-20 (2047, 512)\n",
      "g2-512-30 (2047, 512)\n",
      "g2-512-40 (2047, 512)\n",
      "g2-512-50 (2047, 512)\n",
      "g2-512-60 (2047, 512)\n",
      "g2-512-70 (2047, 512)\n",
      "g2-512-80 (2047, 512)\n",
      "g2-512-90 (2047, 512)\n",
      "g2-64-10 (2047, 64)\n",
      "g2-64-100 (2047, 64)\n",
      "g2-64-20 (2047, 64)\n",
      "g2-64-30 (2047, 64)\n",
      "g2-64-40 (2047, 64)\n",
      "g2-64-50 (2047, 64)\n",
      "g2-64-60 (2047, 64)\n",
      "g2-64-70 (2047, 64)\n",
      "g2-64-80 (2047, 64)\n",
      "g2-64-90 (2047, 64)\n",
      "g2-8-10 (2047, 8)\n",
      "g2-8-100 (2047, 8)\n",
      "g2-8-20 (2047, 8)\n",
      "g2-8-30 (2047, 8)\n",
      "g2-8-40 (2047, 8)\n",
      "g2-8-50 (2047, 8)\n",
      "g2-8-60 (2047, 8)\n",
      "g2-8-70 (2047, 8)\n",
      "g2-8-80 (2047, 8)\n",
      "g2-8-90 (2047, 8)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "import warnings\n",
    "\n",
    "import os\n",
    "from sklearn.cluster import KMeans, DBSCAN, AffinityPropagation, Birch, OPTICS, MeanShift, AgglomerativeClustering\n",
    "import time\n",
    "from sklearn.model_selection import ParameterSampler\n",
    "from typing import Union, Tuple\n",
    "from os import PathLike\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix\n",
    "from scipy.optimize import linear_sum_assignment\n",
    "from sklearn import metrics\n",
    "import ast\n",
    "\n",
    "n_iter = 200\n",
    "MAJOR_MINOR_VERSION = '1.3'\n",
    "\n",
    "\n",
    "def evaluate_clustering(X, labels_true, labels_pred, clus_algo_name, dataset_name, results_path, algorithm_details,\n",
    "                        training_time, prediction_time):\n",
    "    \"\"\"\n",
    "    Evaluates the clustering performance using various metrics and saves the results to a CSV file.\n",
    "\n",
    "    :param X: Feature set.\n",
    "    :param labels_true: Ground truth labels.\n",
    "    :param labels_pred: Predicted cluster labels.\n",
    "    :param clus_algo_name: Name of the clustering algorithm.\n",
    "    :param dataset_name: Name of the dataset.\n",
    "    :param results_path: Path to save the results CSV file.\n",
    "    \"\"\"\n",
    "    # Ensure there are at least 2 unique labels before calculating certain metrics\n",
    "    unique_labels = np.unique(labels_pred)\n",
    "\n",
    "    # Initialize default values for scores that require multiple clusters\n",
    "    calinski_harabasz_score = np.nan\n",
    "    davies_bouldin_score = np.nan\n",
    "    silhouette_score = np.nan\n",
    "\n",
    "    if len(unique_labels) > 1:\n",
    "        calinski_harabasz_score = metrics.calinski_harabasz_score(X, labels_pred)\n",
    "        davies_bouldin_score = metrics.davies_bouldin_score(X, labels_pred)\n",
    "        silhouette_score = metrics.silhouette_score(X, labels_pred)\n",
    "\n",
    "    results = {\n",
    "        'Timestamp': datetime.now(),\n",
    "        'Dataset': dataset_name,\n",
    "        'Clustering Algorithm': clus_algo_name,\n",
    "        'Algorithm Details': algorithm_details,\n",
    "        'Training Time': training_time,\n",
    "        'Prediction Time': prediction_time,\n",
    "        'AMI': metrics.adjusted_mutual_info_score(labels_true, labels_pred),\n",
    "        'ARI': metrics.adjusted_rand_score(labels_true, labels_pred),\n",
    "        'Calinski-Harabasz Score': calinski_harabasz_score,\n",
    "        'Davies-Bouldin Score': davies_bouldin_score,\n",
    "        'Completeness Score': metrics.completeness_score(labels_true, labels_pred),\n",
    "        'Fowlkes-Mallows Score': metrics.fowlkes_mallows_score(labels_true, labels_pred),\n",
    "        'Homogeneity': metrics.homogeneity_score(labels_true, labels_pred),\n",
    "        'Completeness': metrics.completeness_score(labels_true, labels_pred),\n",
    "        'V-Measure': metrics.v_measure_score(labels_true, labels_pred),\n",
    "        'Mutual Information': metrics.mutual_info_score(labels_true, labels_pred),\n",
    "        'Normalized Mutual Information': metrics.normalized_mutual_info_score(labels_true, labels_pred),\n",
    "        'Silhouette Score': silhouette_score,\n",
    "        'Accuracy': accuracy_score(labels_true, labels_pred)\n",
    "\n",
    "    }\n",
    "\n",
    "    # # Print results\n",
    "    # for key, value in results.items():\n",
    "    #     if key == 'Confusion Matrix':\n",
    "    #         print(f\"{key}:\\n{value}\")\n",
    "    #     else:\n",
    "    #         print(f\"{key}: {value}\")\n",
    "\n",
    "    # Save to CSV\n",
    "    df = pd.DataFrame([results])\n",
    "    df.to_csv(results_path, mode='a', header=not os.path.exists(results_path), index=False)\n",
    "\n",
    "\n",
    "def map_clusters_to_ground_truth(labels_true, labels_pred):\n",
    "    \"\"\"\n",
    "    Maps clustering algorithm output to ground truth labels using the Hungarian algorithm.\n",
    "\n",
    "    :param labels_true: Ground truth labels.\n",
    "    :param labels_pred: Predicted cluster labels.\n",
    "    :return: Remapped predicted labels.\n",
    "    \"\"\"\n",
    "    # Calculate the confusion matrix\n",
    "    cm = confusion_matrix(labels_true, labels_pred)\n",
    "    # Apply the Hungarian algorithm to the negative confusion matrix for maximum matching\n",
    "    row_ind, col_ind = linear_sum_assignment(-cm)\n",
    "\n",
    "    # Create a new array to hold the remapped predicted labels\n",
    "    remapped_labels_pred = np.zeros_like(labels_pred)\n",
    "    # For each original cluster index, find the new label (according to the Hungarian algorithm)\n",
    "    # and assign it in the remapped labels array\n",
    "    for original_cluster, new_label in zip(col_ind, row_ind):\n",
    "        remapped_labels_pred[labels_pred == original_cluster] = new_label\n",
    "\n",
    "    return remapped_labels_pred\n",
    "\n",
    "\n",
    "def load_labels_from_file(file_path, labels_pred_len):\n",
    "    \"\"\"\n",
    "    Loads clustering labels from a text file, ignoring the header and metadata.\n",
    "\n",
    "    :param file_path: Path to the file containing the labels.\n",
    "    :return: List of labels as integers.\n",
    "    \"\"\"\n",
    "    with open(file_path, 'r') as file:\n",
    "        lines = file.readlines()\n",
    "\n",
    "    # Skipping the header and metadata, start reading from the line after '-----'\n",
    "    start_index = lines.index('-------------------------------------\\n') + 1\n",
    "    labels_true = [int(line.strip()) for line in lines[start_index:]]\n",
    "    # if labels_pred_len != len(labels_true):\n",
    "    #     raise ValueError(\n",
    "    #         f\"This is a custom error raised by the developer./ Please check the file {file_path} or labels \"\n",
    "    #         f\"definition.\")\n",
    "\n",
    "    return labels_true\n",
    "\n",
    "\n",
    "def preprocess_data(df):\n",
    "    \"\"\"\n",
    "    Function to preprocess data by normalizing it.\n",
    "\n",
    "    :param df: pandas DataFrame with raw data.\n",
    "    :return: pandas DataFrame with processed (normalized) data.\n",
    "    \"\"\"\n",
    "    processed_df = df.copy()\n",
    "\n",
    "    # Normalize the data\n",
    "    # For each column, subtract the minimum and divide by the range.\n",
    "    for column in processed_df.columns:\n",
    "        min_value = processed_df[column].min()\n",
    "        max_value = processed_df[column].max()\n",
    "        processed_df[column] = (processed_df[column] - min_value) / (max_value - min_value)\n",
    "\n",
    "    return processed_df\n",
    "\n",
    "\n",
    "def find_next_version_number(base_path: str, algorithm_name: str, major_minor_version: str) -> str:\n",
    "    \"\"\"\n",
    "    Finds the next version number based on existing directories within the algorithm name directory.\n",
    "    Args:\n",
    "    - base_path: The base directory path where algorithms are stored.\n",
    "    - algorithm_name: The name of the algorithm.\n",
    "    - major_minor_version: The major and minor version components (e.g., '1.0').\n",
    "\n",
    "    Returns:\n",
    "    - The next version as a string (e.g., '1.0.3').\n",
    "    \"\"\"\n",
    "    algorithm_path = os.path.join(base_path, algorithm_name)\n",
    "    if not os.path.exists(algorithm_path):\n",
    "        return major_minor_version + '.1'  # Start with version .1 if no directory exists\n",
    "\n",
    "    # List all version directories and filter by the major_minor_version prefix\n",
    "    version_dirs = [d for d in os.listdir(algorithm_path) if os.path.isdir(os.path.join(algorithm_path, d))]\n",
    "    version_nums = [d.replace(major_minor_version + '.', '') for d in version_dirs if d.startswith(major_minor_version)]\n",
    "\n",
    "    if not version_nums:\n",
    "        return major_minor_version + '.1'  # Start with version .1 if no matching directories\n",
    "\n",
    "    # Find the highest current version number\n",
    "    latest_version = max([int(num) for num in version_nums if num.isdigit()], default=0)\n",
    "\n",
    "    # Return the next version number\n",
    "    return major_minor_version + '.' + str(latest_version + 1)\n",
    "\n",
    "\n",
    "# Function to create directories and return the path for results\n",
    "def create_dirs_and_get_results_path(base_path: Union[str, PathLike],\n",
    "                                     algorithm_name: str,\n",
    "                                     version: str,\n",
    "                                     dataset_dir: str,\n",
    "                                     dataset_name: str,\n",
    "                                     filename: str) -> str:\n",
    "    directory_path = os.path.join(base_path, algorithm_name, version, dataset_dir, dataset_name)\n",
    "    os.makedirs(directory_path, exist_ok=True)\n",
    "    return os.path.join(directory_path, filename)\n",
    "\n",
    "\n",
    "def calculate_grid_size(space):\n",
    "    # For each parameter, count the number of unique values and multiply them\n",
    "    return np.prod([len(values) for values in space.values()])\n",
    "\n",
    "\n",
    "# Functions\n",
    "def train_and_time(clustering_model, train_data):\n",
    "    \"\"\"Train the model and measure training time.\"\"\"\n",
    "    start_time = time.time()\n",
    "    clustering_model.fit(train_data)\n",
    "    end_time = time.time()\n",
    "    return clustering_model, end_time - start_time\n",
    "\n",
    "\n",
    "def predict_and_time(clustering_model, data):\n",
    "    \"\"\"Predict using the model and measure prediction time.\"\"\"\n",
    "    start_time = time.time()\n",
    "    if ALGORITHM_NAME == 'DBSCAN' or ALGORITHM_NAME == 'OPTICS' or ALGORITHM_NAME == 'AgglomerativeClustering':\n",
    "        labels_pred = clustering_model.fit_predict(data)\n",
    "    else:\n",
    "        labels_pred = clustering_model.predict(data)\n",
    "    end_time = time.time()\n",
    "    return labels_pred, end_time - start_time\n",
    "\n",
    "\n",
    "def evaluate_and_log(clustering_model, X_train, X_validate, labels_true, results_path):\n",
    "    \"\"\"Evaluate the clustering and log the results.\"\"\"\n",
    "    algorithm_details = str(clustering_model.get_params())\n",
    "    _, training_time = train_and_time(clustering_model, X_train)\n",
    "    labels_pred, prediction_time = predict_and_time(clustering_model, X_validate)\n",
    "    labels_pred = map_clusters_to_ground_truth(labels_true, labels_pred)\n",
    "    evaluate_clustering(X=X_validate, labels_true=labels_true, labels_pred=labels_pred,\n",
    "                        clus_algo_name=ALGORITHM_NAME, dataset_name=DATASET_FILE_NAME,\n",
    "                        results_path=results_path, algorithm_details=algorithm_details,\n",
    "                        training_time=training_time, prediction_time=prediction_time)\n",
    "\n",
    "\n",
    "# N_DIMENSIONS = 2\n",
    "# N_CLUSTERS = 15\n",
    "\n",
    "# Exponent value generation technique\n",
    "# Generate 20 random exponent values uniformly between -4 and 1 for DBSCAN's eps\n",
    "eps_exponents = np.random.uniform(-7, 1, 20000)\n",
    "eps_values = 10 ** eps_exponents\n",
    "\n",
    "# For AffinityPropagation's 'preference'\n",
    "preference_exponents = np.random.uniform(-2, 1, 50)  # Choosing an exponent range\n",
    "preference_values = 10 ** preference_exponents\n",
    "\n",
    "# For BIRCH's 'threshold'\n",
    "threshold_exponents = np.random.uniform(-1, 0, 30)  # Generating values between 10^-1 and 10^0\n",
    "threshold_values = 10 ** threshold_exponents\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "# current_directory = os.path.join('/opt', 'home', 's3934056')\n",
    "current_directory = '..'\n",
    "DATASET_DIRS = [\n",
    "        # 'A-sets',\n",
    "        # 'Birch-sets',\n",
    "        # 'DIM-sets-high',\n",
    "        'G2-sets', \n",
    "        # 'S-sets',\n",
    "        # 'Unbalance'\n",
    "        ]\n",
    "# DATASET_DIRS = ['A-sets']\n",
    "\n",
    "\n",
    "\n",
    "metrics_file_path = os.path.join(current_directory, 'results', 'metrics')\n",
    "# Determine the next version number automatically\n",
    "\n",
    "\n",
    "algorithms = [\n",
    "        # 'KMeans', \n",
    "        'DBSCAN',\n",
    "        # 'AffinityPropagation', \n",
    "        # 'BIRCH', \n",
    "        # 'OPTICS',\n",
    "        #  'Mean Shift',\n",
    "        # 'AgglomerativeClustering'\n",
    "        ]\n",
    "# algorithms = ['KMeans']\n",
    "\n",
    "for algorithm_name in algorithms:\n",
    "    ALGORITHM_NAME = algorithm_name\n",
    "    VERSION = find_next_version_number(metrics_file_path, ALGORITHM_NAME, MAJOR_MINOR_VERSION)\n",
    "    for DATASET_DIR in DATASET_DIRS:\n",
    "        # Specify the raw and processed data directory paths\n",
    "        # raw_directory_path = f'data/raw/{DATASET_DIR}'\n",
    "        raw_directory_path = os.path.join(current_directory, 'data', 'raw', DATASET_DIR)\n",
    "        # processed_directory_path = f'data/processed/{DATASET_DIR}'\n",
    "        processed_directory_path = os.path.join(current_directory, 'data', 'processed', DATASET_DIR)\n",
    "\n",
    "        os.makedirs(processed_directory_path, exist_ok=True)  # This creates the directory if it does not exist\n",
    "\n",
    "        # Get a list of all files in the directory (excluding directories)\n",
    "        files = [f for f in os.listdir(raw_directory_path) if os.path.isfile(os.path.join(raw_directory_path, f))]\n",
    "        total_files = len(files)\n",
    "        # \n",
    "        # # Run preprocessing step for all files in the specified directory\n",
    "        # for index, filename in enumerate(files, start=1):\n",
    "        #     FILE_NAME = filename.split('.')[0]\n",
    "        #     raw_file_path = os.path.join(raw_directory_path, f'{FILE_NAME}.txt')\n",
    "        #     processed_file_path = os.path.join(processed_directory_path, f'{FILE_NAME}.txt')\n",
    "        # \n",
    "        #     # Check if the processed file already exists\n",
    "        #     if not os.path.isfile(processed_file_path):\n",
    "        #         # print(f\"[{index}/{total_files}] Processing {FILE_NAME}\")\n",
    "        # \n",
    "        #         # The regular expression '\\s+' can be used to match one or more spaces\n",
    "        #         data = pd.read_csv(raw_file_path, sep=\"\\s+\", header=None, names=['X', 'Y'])\n",
    "        #         # Remove rows with missing values:\n",
    "        #         # data_clean = data.dropna()\n",
    "        #         print(data[1])\n",
    "        #         processed_data = preprocess_data(data)\n",
    "        # \n",
    "        #         # Save the processed data to a CSV file\n",
    "        #         processed_data.to_csv(processed_file_path, index=False)\n",
    "\n",
    "        # Run clustering algorithm for all files in the specified directory\n",
    "        for index, filename in enumerate(files, start=1):\n",
    "            if os.path.isfile(os.path.join(raw_directory_path, filename)):\n",
    "                # print(f\"[{index}/{total_files}] {filename.split('.')[0]}\")\n",
    "\n",
    "                DATASET_FILE_NAME = filename.split('.')[0]\n",
    "                LABELS_FILE_NAME = f'{DATASET_FILE_NAME}-gt.pa'\n",
    "\n",
    "                # # Read labels\n",
    "                # labels_true = load_labels_from_file(\n",
    "                #     os.path.join(current_directory, 'data', 'label', DATASET_DIR, LABELS_FILE_NAME), 15)\n",
    "\n",
    "                # Get the number of clusters form the ground truth\n",
    "                # N_CLUSTERS = len(set(labels_true))\n",
    "                raw_file_path = os.path.join(current_directory, 'data', 'raw', DATASET_DIR, f'{DATASET_FILE_NAME}.txt')\n",
    "\n",
    "                raw_data = pd.read_csv(raw_file_path, sep='\\s+',)\n",
    "                N_DIMENSIONS = raw_data.shape[1]\n",
    "                print(DATASET_FILE_NAME, raw_data.shape)\n",
    "\n",
    "                hyperparameter_domains = {\n",
    "                    'KMeans': {\n",
    "                        'n_clusters': [N_CLUSTERS],\n",
    "                        'init': ['k-means++', 'random'],\n",
    "                        'n_init': [5, 10, 20, 30, 40, 50, 60, 70, 80, 90],\n",
    "                        'max_iter': [100, 300, 1000, 1500, 2000, 3000, 4000, 5000],\n",
    "                        'algorithm': ['auto', 'elkan', 'full']\n",
    "                    },\n",
    "                    'DBSCAN': {\n",
    "                        'eps': eps_values,  # Generated eps_values\n",
    "                        'min_samples': [N_DIMENSIONS, N_DIMENSIONS+1]\n",
    "                        # Adjusting to N_DIMENSIONS + 1 based on dimensionality\n",
    "                    },\n",
    "                    'AffinityPropagation': {\n",
    "                        'damping': [0.9],\n",
    "                        'preference': [-0.5],\n",
    "                    },\n",
    "                    'BIRCH': {\n",
    "                        'threshold': threshold_values,  # log-scale generated values\n",
    "                        'branching_factor': list(range(10, 100, 5)),  # Keeping linear steps\n",
    "                        'n_clusters': [N_CLUSTERS]\n",
    "                    },\n",
    "                    # 'Fuzzy C Means': {\n",
    "                    #     'tolerance': [1e-4, 1e-3, 1e-2],\n",
    "                    #     'itermax': list(range(100, 1000, 100)),\n",
    "                    #     'm': np.linspace(1.1, 2.0, 10)\n",
    "                    # },\n",
    "                    'OPTICS': {\n",
    "                        'min_samples': [10, 20, 50],\n",
    "                        'max_eps': [1, 5, 7.5, np.inf],\n",
    "                        'metric': ['euclidean', 'cosine'],\n",
    "                        'xi': np.linspace(0.01, 0.1, 5),\n",
    "                        'min_cluster_size': [None, 5, 10, 20]\n",
    "                    },\n",
    "                    'Mean Shift': {\n",
    "                        'bandwidth': [0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7],\n",
    "                        'bin_seeding': [True, False],\n",
    "                        'min_bin_freq': [5, 10, 20],\n",
    "                        'cluster_all': [True, False],\n",
    "                        'max_iter': [100, 300, 1000, 5000]\n",
    "                    },\n",
    "                    'AgglomerativeClustering': {\n",
    "                        'n_clusters': [N_CLUSTERS],\n",
    "                        'affinity': ['euclidean'],\n",
    "                        'linkage': ['ward']\n",
    "                    },\n",
    "                    'BisectingKMeans': {\n",
    "                        'n_clusters': [N_CLUSTERS],\n",
    "                        'init': ['k-means++'],\n",
    "                        'n_init': [1, 10, 15, 20],\n",
    "                        'tol': [1e-4, 1e-3, 1e-2],\n",
    "                        'max_iter': [50, 100, 300, 1000, 5000],\n",
    "                        'algorithm': ['lloyd'],\n",
    "                        'bisecting_strategy': ['biggest_inertia', 'biggest_cluster']\n",
    "                    },\n",
    "                    # 'CURE': {\n",
    "                    #     'number_cluster': [N_CLUSTERS],\n",
    "                    #     'number_represent_points': list(range(1, 15)),\n",
    "                    #     'compression': np.linspace(0.05, 0.95, 19)\n",
    "                    # },\n",
    "                    # 'CLIQUE': {\n",
    "                    #     'amount_intervals': list(range(2, 20)),\n",
    "                    #     'density_threshold': list(range(1, 10))\n",
    "                    # }\n",
    "                }\n",
    "\n",
    "                # Read processed data\n",
    "                # processed_file_path = rf'data\\processed\\{DATASET_DIR}\\{DATASET_FILE_NAME}.txt'\n",
    "                # processed_file_path = os.path.join(current_directory, 'data', 'processed', DATASET_DIR,\n",
    "                #                                    f'{DATASET_FILE_NAME}.txt')\n",
    "                # processed_data = pd.read_csv(processed_file_path)\n",
    "                #\n",
    "\n",
    "\n",
    "        #         # Split the data into train and temp (temp will contain both validate and test)\n",
    "        #         train_data, temp_data, train_labels, temp_labels = train_test_split(\n",
    "        #             processed_data, labels_true, train_size=0.5, random_state=42)\n",
    "        #\n",
    "        #         # Split the temp data into validate and test\n",
    "        #         validate_data, test_data, validate_labels, test_labels = train_test_split(\n",
    "        #             temp_data, temp_labels, train_size=0.5, random_state=42)\n",
    "        #\n",
    "        #         hyperparameter_domain = hyperparameter_domains[algorithm_name]\n",
    "        #\n",
    "        #         grid_size = calculate_grid_size(hyperparameter_domain)\n",
    "        #\n",
    "        #         # Adjusted for creating directories and files accordingly\n",
    "        #         # For Hyperparameter Tuning Logs\n",
    "        #         tuning_results_filename = 'hyperparameter_tuning_logs.csv'\n",
    "        #         tuning_results_path = create_dirs_and_get_results_path(metrics_file_path, ALGORITHM_NAME, VERSION,\n",
    "        #                                                                DATASET_DIR, DATASET_FILE_NAME,\n",
    "        #                                                                tuning_results_filename)\n",
    "        #\n",
    "        #         # For Final Results\n",
    "        #         final_results_filename = 'results.csv'\n",
    "        #         final_results_path = create_dirs_and_get_results_path(metrics_file_path, ALGORITHM_NAME, VERSION,\n",
    "        #                                                               DATASET_DIR, DATASET_FILE_NAME,\n",
    "        #                                                               final_results_filename)\n",
    "        #\n",
    "        #         parameter_sampler = ParameterSampler(hyperparameter_domain, n_iter=n_iter, random_state=42)\n",
    "        #\n",
    "        #         # Run clustering for each configuration\n",
    "        #         for i, params in enumerate(parameter_sampler, start=1):\n",
    "        #             # print(f\"Running configuration {i}/{n_iter}: {params}\")  # Debugging: print before running\n",
    "        #             start_time = time.time()\n",
    "        #\n",
    "        #             if algorithm_name == 'KMeans':\n",
    "        #                 model = KMeans(**params)\n",
    "        #\n",
    "        #             elif algorithm_name == 'DBSCAN':\n",
    "        #                 model = DBSCAN(**params)\n",
    "        #\n",
    "        #             elif algorithm_name == 'AffinityPropagation':\n",
    "        #                 model = AffinityPropagation(**params)\n",
    "        #\n",
    "        #             elif algorithm_name == 'BIRCH':\n",
    "        #                 model = Birch(**params)\n",
    "        #\n",
    "        #             elif algorithm_name == 'OPTICS':\n",
    "        #                 model = OPTICS(**params)\n",
    "        #\n",
    "        #             elif algorithm_name == 'Mean Shift':\n",
    "        #                 model = MeanShift(**params)\n",
    "        #\n",
    "        #             elif algorithm_name == 'AgglomerativeClustering':\n",
    "        #                 model = AgglomerativeClustering(**params)\n",
    "        #\n",
    "        #             else:\n",
    "        #                 raise ValueError(\"Unsupported algorithm\")\n",
    "        #             evaluate_and_log(model, train_data, validate_data, validate_labels, tuning_results_path)\n",
    "        #\n",
    "        #         # Read hyperparameter tuning logs\n",
    "        #         csv_content = pd.read_csv(tuning_results_path)\n",
    "        #\n",
    "        #         # Find the record with the highest accuracy\n",
    "        #         max_accuracy_record = csv_content.loc[csv_content['Accuracy'].idxmax()]\n",
    "        #\n",
    "        #         # Display the record with the highest accuracy\n",
    "        #         # print(max_accuracy_record)\n",
    "        #         # print(max_accuracy_record['Algorithm Details'])\n",
    "        #         # print(max_accuracy_record['Accuracy'], \"\\n\")\n",
    "        #\n",
    "        #         combined_train_data = pd.concat([train_data, validate_data])\n",
    "        #         combined_train_labels = np.concatenate([train_labels, validate_labels])\n",
    "        #\n",
    "        #         # Initialize KMeans with the best hyperparameters\n",
    "        #         best_params = max_accuracy_record['Algorithm Details']  # Assume this contains the best parameters\n",
    "        #         if algorithm_name == 'KMeans':\n",
    "        #             model_final = KMeans(**eval(best_params))\n",
    "        #\n",
    "        #         elif algorithm_name == 'DBSCAN':\n",
    "        #             model_final = DBSCAN(**eval(best_params))\n",
    "        #\n",
    "        #         elif algorithm_name == 'AffinityPropagation':\n",
    "        #             model_final = AffinityPropagation(**eval(best_params))\n",
    "        #\n",
    "        #         elif algorithm_name == 'BIRCH':\n",
    "        #             model_final = Birch(**eval(best_params))\n",
    "        #\n",
    "        #         elif algorithm_name == 'OPTICS':\n",
    "        #             inf = float('inf')\n",
    "        #             model_final = OPTICS(**eval(best_params))\n",
    "        #\n",
    "        #         elif algorithm_name == 'Mean Shift':\n",
    "        #             model_final = MeanShift(**eval(best_params))\n",
    "        #\n",
    "        #         elif algorithm_name == 'AgglomerativeClustering':\n",
    "        #             model_final = AgglomerativeClustering(**eval(best_params))\n",
    "        #\n",
    "        #         else:\n",
    "        #             raise ValueError(\"Unsupported algorithm\")\n",
    "        #\n",
    "        #         evaluate_and_log(model_final, combined_train_data, test_data, test_labels, final_results_path)\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-24T11:46:20.967662400Z",
     "start_time": "2024-02-24T11:46:09.186229700Z"
    }
   },
   "id": "21eae5c430f9e30d",
   "execution_count": 64
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
