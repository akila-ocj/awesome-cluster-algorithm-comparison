{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-04-03T11:48:52.374484100Z",
     "start_time": "2024-04-03T11:48:48.253717200Z"
    }
   },
   "outputs": [],
   "source": [
    "import warnings\n",
    "from sklearn.model_selection import train_test_split\n",
    "import os\n",
    "from sklearn.cluster import KMeans, DBSCAN, AffinityPropagation, Birch, OPTICS, MeanShift, AgglomerativeClustering\n",
    "import time\n",
    "from sklearn.model_selection import ParameterSampler\n",
    "from typing import Union, Tuple\n",
    "from os import PathLike\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix\n",
    "from scipy.optimize import linear_sum_assignment\n",
    "from sklearn import metrics\n",
    "from enum import Enum, auto"
   ]
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "MAJOR_MINOR_VERSION = '2.0'"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-03T11:48:52.381191Z",
     "start_time": "2024-04-03T11:48:52.376343400Z"
    }
   },
   "id": "25d4962e6d18770",
   "execution_count": 2
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "def evaluate_clustering(X, labels_true, labels_pred, clus_algo_name, dataset_name, results_path, algorithm_details,\n",
    "                        training_time, prediction_time, drift_type, drift_level):\n",
    "    \"\"\"\n",
    "    Evaluates the clustering performance using various metrics and saves the results to a CSV file.\n",
    "\n",
    "    :param X: Feature set.\n",
    "    :param labels_true: Ground truth labels.\n",
    "    :param labels_pred: Predicted cluster labels.\n",
    "    :param clus_algo_name: Name of the clustering algorithm.\n",
    "    :param dataset_name: Name of the dataset.\n",
    "    :param results_path: Path to save the results CSV file.\n",
    "    \"\"\"\n",
    "    # Ensure there are at least 2 unique labels before calculating certain metrics\n",
    "    unique_labels = np.unique(labels_pred)\n",
    "\n",
    "    # Initialize default values for scores that require multiple clusters\n",
    "    calinski_harabasz_score = np.nan\n",
    "    davies_bouldin_score = np.nan\n",
    "    silhouette_score = np.nan\n",
    "\n",
    "    if len(unique_labels) > 1:\n",
    "        calinski_harabasz_score = metrics.calinski_harabasz_score(X, labels_pred)\n",
    "        davies_bouldin_score = metrics.davies_bouldin_score(X, labels_pred)\n",
    "        silhouette_score = metrics.silhouette_score(X, labels_pred)\n",
    "\n",
    "    results = {\n",
    "        'Timestamp': datetime.now(),\n",
    "        'Dataset': dataset_name,\n",
    "        'Clustering Algorithm': clus_algo_name,\n",
    "        'Algorithm Details': algorithm_details,\n",
    "        'Drift Type': drift_type,\n",
    "        'Drift Level': drift_level,\n",
    "        'Training Time': training_time,\n",
    "        'Prediction Time': prediction_time,\n",
    "        'AMI': metrics.adjusted_mutual_info_score(labels_true, labels_pred),\n",
    "        'ARI': metrics.adjusted_rand_score(labels_true, labels_pred),\n",
    "        'Calinski-Harabasz Score': calinski_harabasz_score,\n",
    "        'Davies-Bouldin Score': davies_bouldin_score,\n",
    "        'Completeness Score': metrics.completeness_score(labels_true, labels_pred),\n",
    "        'Fowlkes-Mallows Score': metrics.fowlkes_mallows_score(labels_true, labels_pred),\n",
    "        'Homogeneity': metrics.homogeneity_score(labels_true, labels_pred),\n",
    "        'Completeness': metrics.completeness_score(labels_true, labels_pred),\n",
    "        'V-Measure': metrics.v_measure_score(labels_true, labels_pred),\n",
    "        'Mutual Information': metrics.mutual_info_score(labels_true, labels_pred),\n",
    "        'Normalized Mutual Information': metrics.normalized_mutual_info_score(labels_true, labels_pred),\n",
    "        'Silhouette Score': silhouette_score,\n",
    "        'Accuracy': accuracy_score(labels_true, labels_pred)\n",
    "\n",
    "    }\n",
    "\n",
    "    # # Print results\n",
    "    # for key, value in results.items():\n",
    "    #     if key == 'Confusion Matrix':\n",
    "    #         print(f\"{key}:\\n{value}\")\n",
    "    #     else:\n",
    "    #         print(f\"{key}: {value}\")\n",
    "\n",
    "    # Save to CSV\n",
    "    df = pd.DataFrame([results])\n",
    "    df.to_csv(results_path, mode='a', header=not os.path.exists(results_path), index=False)\n",
    "\n",
    "\n",
    "def map_clusters_to_ground_truth(labels_true, labels_pred):\n",
    "    \"\"\"\n",
    "    Maps clustering algorithm output to ground truth labels using the Hungarian algorithm.\n",
    "\n",
    "    :param labels_true: Ground truth labels.\n",
    "    :param labels_pred: Predicted cluster labels.\n",
    "    :return: Remapped predicted labels.\n",
    "    \"\"\"\n",
    "    # Calculate the confusion matrix\n",
    "    cm = confusion_matrix(labels_true, labels_pred)\n",
    "    # Apply the Hungarian algorithm to the negative confusion matrix for maximum matching\n",
    "    row_ind, col_ind = linear_sum_assignment(-cm)\n",
    "\n",
    "    # Create a new array to hold the remapped predicted labels\n",
    "    remapped_labels_pred = np.zeros_like(labels_pred)\n",
    "    # For each original cluster index, find the new label (according to the Hungarian algorithm)\n",
    "    # and assign it in the remapped labels array\n",
    "    for original_cluster, new_label in zip(col_ind, row_ind):\n",
    "        remapped_labels_pred[labels_pred == original_cluster] = new_label\n",
    "\n",
    "    return remapped_labels_pred\n",
    "\n",
    "\n",
    "def load_labels_from_file(file_path, labels_pred_len):\n",
    "    \"\"\"\n",
    "    Loads clustering labels from a text file, ignoring the header and metadata.\n",
    "\n",
    "    :param file_path: Path to the file containing the labels.\n",
    "    :return: List of labels as integers.\n",
    "    \"\"\"\n",
    "    with open(file_path, 'r') as file:\n",
    "        lines = file.readlines()\n",
    "\n",
    "    # Skipping the header and metadata, start reading from the line after '-----'\n",
    "    start_index = lines.index('-------------------------------------\\n') + 1\n",
    "    labels_true = [int(line.strip()) for line in lines[start_index:]]\n",
    "    # if labels_pred_len != len(labels_true):\n",
    "    #     raise ValueError(\n",
    "    #         f\"This is a custom error raised by the developer./ Please check the file {file_path} or labels \"\n",
    "    #         f\"definition.\")\n",
    "\n",
    "    return labels_true\n",
    "\n",
    "\n",
    "def preprocess_data(df):\n",
    "    \"\"\"\n",
    "    Function to preprocess data by normalizing it.\n",
    "\n",
    "    :param df: pandas DataFrame with raw data.\n",
    "    :return: pandas DataFrame with processed (normalized) data.\n",
    "    \"\"\"\n",
    "    processed_df = df.copy()\n",
    "\n",
    "    # Normalize the data\n",
    "    # For each column, subtract the minimum and divide by the range.\n",
    "    for column in processed_df.columns:\n",
    "        min_value = processed_df[column].min()\n",
    "        max_value = processed_df[column].max()\n",
    "        processed_df[column] = (processed_df[column] - min_value) / (max_value - min_value)\n",
    "\n",
    "    return processed_df\n",
    "\n",
    "\n",
    "# Function to create directories and return the path for results\n",
    "def create_dirs_and_get_results_path(base_path: Union[str, PathLike],\n",
    "                                     algorithm_name: str,\n",
    "                                     version: str,\n",
    "                                     dataset_dir: str,\n",
    "                                     dataset_name: str,\n",
    "                                     filename: str) -> str:\n",
    "    directory_path = os.path.join(base_path, algorithm_name, version, dataset_dir, dataset_name)\n",
    "    os.makedirs(directory_path, exist_ok=True)\n",
    "    return os.path.join(directory_path, filename)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-03T11:48:52.387500600Z",
     "start_time": "2024-04-03T11:48:52.378191400Z"
    }
   },
   "id": "b7a488fd93a971b8",
   "execution_count": 3
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "def train_and_time(clustering_model, train_data):\n",
    "    \"\"\"Train the model and measure training time.\"\"\"\n",
    "    start_time = time.time()\n",
    "    clustering_model.fit(train_data)\n",
    "    end_time = time.time()\n",
    "    return clustering_model, end_time - start_time\n",
    "\n",
    "\n",
    "def predict_and_time(clustering_model, data):\n",
    "    \"\"\"Predict using the model and measure prediction time.\"\"\"\n",
    "    start_time = time.time()\n",
    "    if ALGORITHM_NAME == 'DBSCAN' or ALGORITHM_NAME == 'OPTICS' or ALGORITHM_NAME == 'AgglomerativeClustering':\n",
    "        labels_pred = clustering_model.fit_predict(data)\n",
    "    else:\n",
    "        labels_pred = clustering_model.predict(data)\n",
    "    end_time = time.time()\n",
    "    return labels_pred, end_time - start_time\n",
    "\n",
    "\n",
    "def evaluate_and_log(clustering_model, X_train, X_validate, labels_true, results_path, drift_type, drift_level):\n",
    "    \"\"\"Evaluate the clustering and log the results.\"\"\"\n",
    "    algorithm_details = str(clustering_model.get_params())\n",
    "    _, training_time = train_and_time(clustering_model, X_train)\n",
    "    labels_pred, prediction_time = predict_and_time(clustering_model, X_validate)\n",
    "    labels_pred = map_clusters_to_ground_truth(labels_true, labels_pred)\n",
    "    evaluate_clustering(X=X_validate, labels_true=labels_true, labels_pred=labels_pred,\n",
    "                        clus_algo_name=ALGORITHM_NAME, dataset_name=DATASET_FILE_NAME,\n",
    "                        results_path=results_path, algorithm_details=algorithm_details,\n",
    "                        training_time=training_time, prediction_time=prediction_time, drift_type=drift_type, drift_level=drift_level)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-03T11:48:52.397004Z",
     "start_time": "2024-04-03T11:48:52.388502100Z"
    }
   },
   "id": "ab161a8fbd78f923",
   "execution_count": 4
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "def find_valid_target_class(test_labels, initial_target_class):\n",
    "    \"\"\"\n",
    "    Finds a valid target class that exists in the test labels.\n",
    "\n",
    "    :param test_labels: Array of test labels.\n",
    "    :param initial_target_class: The initial target class to start searching from.\n",
    "    :return: A valid target class that exists in the test labels.\n",
    "    \"\"\"\n",
    "    unique_classes = np.unique(test_labels)\n",
    "    target_class = initial_target_class\n",
    "    while target_class not in unique_classes:\n",
    "        target_class += 1  # Increment target class\n",
    "    return target_class\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "4d7198ab842b9f9b"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "warnings.filterwarnings(\"ignore\")\n",
    "current_directory = os.path.join('/opt', 'home', 's3934056')\n",
    "DATASET_DIRS = [\n",
    "    'A-sets', \n",
    "    'Birch-sets', \n",
    "    'DIM-sets-high', \n",
    "    # 'G2-sets', \n",
    "    # 'S-sets',\n",
    "    # 'Unbalance'\n",
    "]\n",
    "algorithms = ['KMeans']\n",
    "\n",
    "metrics_file_path = os.path.join(current_directory, 'results', 'metrics')"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-03T11:48:52.401664400Z",
     "start_time": "2024-04-03T11:48:52.395983500Z"
    }
   },
   "id": "15630f1f24155d7b",
   "execution_count": 5
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "class DriftTypes(Enum):\n",
    "    KNOCKOUT = 'Knock-out'\n",
    "    GAUSSIAN = 'Gaussian Shift'\n",
    "    CONCEPT = 'Concept Drift'\n",
    "    COVARIANT = 'Covariant Drift'\n",
    "\n",
    "class DriftLevels(Enum):\n",
    "    MILD = 0.01\n",
    "    MODERATE = 0.05\n",
    "    SEVERE = 0.1\n",
    "\n",
    "drift_type = DriftTypes.GAUSSIAN.value"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "305986345333482e"
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[WinError 3] The system cannot find the path specified: '/opt\\\\home\\\\s3934056\\\\data\\\\raw\\\\S-sets'",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mFileNotFoundError\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[7], line 14\u001B[0m\n\u001B[0;32m     11\u001B[0m os\u001B[38;5;241m.\u001B[39mmakedirs(processed_directory_path, exist_ok\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mTrue\u001B[39;00m)  \u001B[38;5;66;03m# This creates the directory if it does not exist\u001B[39;00m\n\u001B[0;32m     13\u001B[0m \u001B[38;5;66;03m# Get a list of all files in the directory (excluding directories)\u001B[39;00m\n\u001B[1;32m---> 14\u001B[0m files \u001B[38;5;241m=\u001B[39m [f \u001B[38;5;28;01mfor\u001B[39;00m f \u001B[38;5;129;01min\u001B[39;00m \u001B[43mos\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mlistdir\u001B[49m\u001B[43m(\u001B[49m\u001B[43mraw_directory_path\u001B[49m\u001B[43m)\u001B[49m \u001B[38;5;28;01mif\u001B[39;00m os\u001B[38;5;241m.\u001B[39mpath\u001B[38;5;241m.\u001B[39misfile(os\u001B[38;5;241m.\u001B[39mpath\u001B[38;5;241m.\u001B[39mjoin(raw_directory_path, f))]\n\u001B[0;32m     15\u001B[0m total_files \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mlen\u001B[39m(files)\n\u001B[0;32m     17\u001B[0m \u001B[38;5;66;03m# Preprocessing\u001B[39;00m\n\u001B[0;32m     18\u001B[0m \u001B[38;5;66;03m# Run preprocessing step for all files in the specified directory\u001B[39;00m\n",
      "\u001B[1;31mFileNotFoundError\u001B[0m: [WinError 3] The system cannot find the path specified: '/opt\\\\home\\\\s3934056\\\\data\\\\raw\\\\S-sets'"
     ]
    }
   ],
   "source": [
    "# Start timing the entire process\n",
    "total_start_time = time.time()\n",
    "\n",
    "for algorithm_name in algorithms:\n",
    "    ALGORITHM_NAME = algorithm_name\n",
    "    VERSION = MAJOR_MINOR_VERSION + '.5'\n",
    "    for DATASET_DIR in DATASET_DIRS:\n",
    "        # Specify the raw and processed data directory paths\n",
    "        # raw_directory_path = f'data/raw/{DATASET_DIR}'\n",
    "        raw_directory_path = os.path.join(current_directory, 'data', 'raw', DATASET_DIR)\n",
    "        # processed_directory_path = f'data/processed/{DATASET_DIR}'\n",
    "        processed_directory_path = os.path.join(current_directory, 'data', 'processed', DATASET_DIR)\n",
    "\n",
    "        os.makedirs(processed_directory_path, exist_ok=True)  # This creates the directory if it does not exist\n",
    "\n",
    "        # Get a list of all files in the directory (excluding directories)\n",
    "        files = [f for f in os.listdir(raw_directory_path) if os.path.isfile(os.path.join(raw_directory_path, f))]\n",
    "        total_files = len(files)\n",
    "\n",
    "        # Preprocessing\n",
    "        # Run preprocessing step for all files in the specified directory\n",
    "        for index, filename in enumerate(files, start=1):\n",
    "            FILE_NAME = filename.split('.')[0]\n",
    "            raw_file_path = os.path.join(raw_directory_path, f'{FILE_NAME}.txt')\n",
    "            processed_file_path = os.path.join(processed_directory_path, f'{FILE_NAME}.txt')\n",
    "\n",
    "            # Check if the processed file already exists\n",
    "            if not os.path.isfile(processed_file_path):\n",
    "                # print(f\"[{index}/{total_files}] Processing {FILE_NAME}\")\n",
    "\n",
    "                # The regular expression '\\s+' can be used to match one or more spaces\n",
    "                data = pd.read_csv(raw_file_path, sep=\"\\s+\", header=None, names=['X', 'Y'])\n",
    "                # Remove rows with missing values:\n",
    "                # data_clean = data.dropna()\n",
    "                processed_data = preprocess_data(data)\n",
    "\n",
    "                # Save the processed data to a CSV file\n",
    "                processed_data.to_csv(processed_file_path, index=False)\n",
    "\n",
    "        # Run clustering algorithm for all files in the specified directory\n",
    "        for index, filename in enumerate(files, start=1):\n",
    "            if os.path.isfile(os.path.join(raw_directory_path, filename)):\n",
    "                # print(f\"[{index}/{total_files}] {filename.split('.')[0]}\")\n",
    "\n",
    "                DATASET_FILE_NAME = filename.split('.')[0]\n",
    "                LABELS_FILE_NAME = f'{DATASET_FILE_NAME}-gt.pa'\n",
    "\n",
    "                # Read labels\n",
    "                labels_true = load_labels_from_file(\n",
    "                    os.path.join(current_directory, 'data', 'label', DATASET_DIR, LABELS_FILE_NAME), 15)\n",
    "\n",
    "                # Get the number of clusters form the ground truth\n",
    "                N_CLUSTERS = len(set(labels_true))\n",
    "\n",
    "                # Read processed data\n",
    "                # processed_file_path = rf'data\\processed\\{DATASET_DIR}\\{DATASET_FILE_NAME}.txt'\n",
    "                processed_file_path = os.path.join(current_directory, 'data', 'processed', DATASET_DIR,\n",
    "                                                   f'{DATASET_FILE_NAME}.txt')\n",
    "                processed_data = pd.read_csv(processed_file_path)\n",
    "\n",
    "                # Split the data into train and temp (temp will contain both validate and test)\n",
    "                train_data, temp_data, train_labels, temp_labels = train_test_split(\n",
    "                    processed_data, labels_true, train_size=0.5, random_state=42)\n",
    "\n",
    "                # Split the temp data into validate and test\n",
    "                validate_data, test_data, validate_labels, test_labels = train_test_split(\n",
    "                    temp_data, temp_labels, train_size=0.5, random_state=42)\n",
    "\n",
    "                # For Hyperparameter Tuning Logs\n",
    "                tuning_results_filename = 'hyperparameter_tuning_logs.csv'\n",
    "                tuning_results_path = create_dirs_and_get_results_path(metrics_file_path, ALGORITHM_NAME, VERSION,\n",
    "                                                                       DATASET_DIR, DATASET_FILE_NAME,\n",
    "                                                                       tuning_results_filename)\n",
    "\n",
    "                # For Final Results\n",
    "                final_results_filename = 'drift_results_v4.csv'\n",
    "                final_results_path = create_dirs_and_get_results_path(metrics_file_path, ALGORITHM_NAME, VERSION,\n",
    "                                                                      DATASET_DIR, DATASET_FILE_NAME,\n",
    "                                                                      final_results_filename)\n",
    "\n",
    "                # Read hyperparameter tuning logs\n",
    "                csv_content = pd.read_csv(tuning_results_path)\n",
    "\n",
    "                # Find the record with the highest accuracy\n",
    "                max_accuracy_record = csv_content.loc[csv_content['Accuracy'].idxmax()]\n",
    "\n",
    "                # # Display the record with the highest accuracy\n",
    "                # print(max_accuracy_record)\n",
    "                # print(max_accuracy_record['Algorithm Details'])\n",
    "                # print(max_accuracy_record['Accuracy'], \"\\n\")\n",
    "\n",
    "                combined_train_data = pd.concat([train_data, validate_data])\n",
    "                combined_train_labels = np.concatenate([train_labels, validate_labels])\n",
    "\n",
    "                # Initialize KMeans with the best hyperparameters\n",
    "                best_params = max_accuracy_record['Algorithm Details']  # Assume this contains the best parameters\n",
    "\n",
    "                model_final = KMeans(**eval(best_params))\n",
    "\n",
    "                # Apply data shift on the test data\n",
    "\n",
    "                # Knock out start\n",
    "                num_splits = 5  # Number of random splits\n",
    "                portion_to_modify = 0.2  # Portion of the dataset to apply drift to\n",
    "\n",
    "                \n",
    "                for level in DriftLevels:                   \n",
    "                    metric_sums = {\n",
    "                        'Accuracy': 0,\n",
    "                        'Silhouette Score': 0\n",
    "                        # Add other metrics here\n",
    "                    }\n",
    "    \n",
    "                    min_accuracy = float('inf')  # Initialize to the highest possible value`               \n",
    "                    \n",
    "                    # In order to qualitatively quantify the robustness of our findings, shift detection performance is averaged over a total of 5 random splits\n",
    "                    for split in range(num_splits):\n",
    "                        std_dev = level.value   # Adjust this value based on your needs 0.01, 0.05, 0.1\n",
    "    \n",
    "                        \n",
    "                        # Randomly select a subset of these indices to remove\n",
    "                        np.random.seed(42 + split)  # For reproducibility\n",
    "                        \n",
    "                        # Calculate the number of samples to select based on the specified portion\n",
    "                        num_samples_to_modify = int(len(test_data) * portion_to_modify)\n",
    "                \n",
    "                        # Randomly select indices to modify\n",
    "                        indices_to_modify = np.random.choice(test_data.index, size=num_samples_to_modify, replace=False)\n",
    "                \n",
    "                        # Make a copy of the test_data to apply drift\n",
    "                        test_data_modified = test_data.copy()\n",
    "                    \n",
    "                        # Apply Gaussian noise as concept drift to the selected indices for both 'X' and 'Y' columns\n",
    "                        test_data_modified.loc[indices_to_modify, 'X'] += np.random.normal(0, std_dev, size=num_samples_to_modify)\n",
    "                        test_data_modified.loc[indices_to_modify, 'Y'] += np.random.normal(0, std_dev, size=num_samples_to_modify)\n",
    "                \n",
    "                        # Evaluate and log with the modified test data\n",
    "                        evaluate_and_log(model_final, combined_train_data, test_data_modified, test_labels, final_results_path, drift_type=drift_type, drift_level=level.name)\n",
    "    \n",
    "                        # Read hyperparameter tuning logs\n",
    "                        csv_content = pd.read_csv(final_results_path)\n",
    "    \n",
    "                        # After evaluation, read the latest record for performance metrics\n",
    "                        latest_record = csv_content.iloc[-1]\n",
    "    \n",
    "                        metric_sums['Accuracy'] += latest_record['Accuracy']\n",
    "                        min_accuracy = min(min_accuracy, latest_record['Accuracy'])\n",
    "                        silhouette_score = latest_record['Silhouette Score']\n",
    "                        metric_sums['Silhouette Score'] += silhouette_score\n",
    "    \n",
    "                    average_metrics = {metric: sum_value / num_splits for metric, sum_value in metric_sums.items()}\n",
    "                    # Display or log the average metrics and the minimum accuracy\n",
    "                    print(\"Drift type: \", drift_type, \"Drift level: \", level)\n",
    "                    for metric, avg_value in average_metrics.items():\n",
    "                        print(f\"{metric}: {avg_value}\")\n",
    "                    print(f\"Minimum Accuracy: {min_accuracy}\")\n",
    "                # Knock out end\n",
    "\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-03T13:00:50.577486400Z",
     "start_time": "2024-04-03T13:00:50.532076300Z"
    }
   },
   "id": "8bdd5cc8c584af12",
   "execution_count": 7
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# End timing the entire process\n",
    "total_end_time = time.time()\n",
    "\n",
    "# Calculate total elapsed time\n",
    "total_elapsed_time = total_end_time - total_start_time\n",
    "\n",
    "# Display the total time taken\n",
    "print(f\"The total process took {total_elapsed_time} seconds.\")"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "24a560edddaab83e"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-03T13:00:50.675573900Z",
     "start_time": "2024-04-03T13:00:50.674565600Z"
    }
   },
   "id": "8f6c6795ca0f4f51",
   "execution_count": 7
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "fc6e3dba285a2bfb"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
